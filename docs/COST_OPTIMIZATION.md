# ğŸ’° LumenAI - Cost Optimization Guide

## Problem: High LLM Costs

LumenAI byÅ‚ domyÅ›lnie skonfigurowany z `gpt-4-turbo-preview` - **najdroÅ¼szym** dostÄ™pnym modelem!

**Koszt przed optymalizacjÄ…:**
- 1 zapytanie = **$0.02-0.03** ğŸ˜±
- 100 zapytaÅ„ dziennie = **$60-90/miesiÄ…c** ğŸ’¸
- 1000 zapytaÅ„ dziennie = **$600-900/miesiÄ…c** ğŸ”¥

## RozwiÄ…zanie: Smart Cost Optimization

LumenAI v1.1 wprowadza **inteligentne zarzÄ…dzanie kosztami** z 75x oszczÄ™dnoÅ›ciami!

### ğŸ¯ Nowe Funkcje

1. **Smart Model Routing** - Automatyczny wybÃ³r najtaÅ„szego modelu do zadania
2. **Response Caching** - Cache odpowiedzi, zero duplikatÃ³w API
3. **Cost Tracking** - Real-time monitoring kosztÃ³w
4. **Token Limits** - Kontrola dÅ‚ugoÅ›ci odpowiedzi
5. **TaÅ„sze domyÅ›lne modele** - gpt-4o-mini zamiast gpt-4-turbo

### ğŸ“Š PorÃ³wnanie Modeli

| Model | Input ($/1M) | Output ($/1M) | Typowy koszt* | Use Case |
|-------|--------------|---------------|---------------|----------|
| **gpt-4o-mini** âœ… | $0.15 | $0.60 | **$0.0004** | DEFAULT - 99% zadaÅ„ |
| gpt-3.5-turbo | $0.50 | $1.50 | $0.0010 | FAST - proste zapytania |
| gpt-4o | $2.50 | $10.00 | $0.0063 | SMART - complex reasoning |
| ~~gpt-4-turbo~~ âŒ | $10.00 | $30.00 | **$0.0200** | NIE UÅ»YWAJ! |

*ZakÅ‚adajÄ…c ~500 input + 500 output tokenÃ³w

### ğŸ’¡ Smart Routing

System automatycznie wybiera model na podstawie zÅ‚oÅ¼onoÅ›ci:

```python
# Proste pytanie â†’ gpt-4o-mini ($0.0004)
"Co to jest Python?"

# Åšrednie â†’ gpt-4o-mini ($0.0004)
"Napisz funkcjÄ™ sortujÄ…cÄ…"

# ZÅ‚oÅ¼one â†’ gpt-4o ($0.0063)
"Zaprojektuj architekturÄ™ systemu ML z analizÄ… trade-offÃ³w..."
```

**OszczÄ™dnoÅ›ci: 75x dla prostych zapytaÅ„!**

## ğŸš€ Quick Start

### 1. Zaktualizuj .env

```bash
# Nowe ustawienia (juÅ¼ w .env.example)
DEFAULT_MODEL=gpt-4o-mini        # 75x taÅ„szy!
SMART_MODEL=gpt-4o               # Dla zÅ‚oÅ¼onych zadaÅ„
FAST_MODEL=gpt-3.5-turbo         # Ultra szybki

ENABLE_SMART_ROUTING=true        # WÅ‚Ä…cz auto-routing
ENABLE_RESPONSE_CACHE=true       # WÅ‚Ä…cz caching
MAX_TOKENS_DEFAULT=1000          # Limit tokenÃ³w
```

### 2. Restart Backend

```bash
# Docker
docker-compose restart backend

# Mamba
mamba activate lumenai
make backend-dev
```

### 3. Gotowe! ğŸ‰

System automatycznie:
- âœ… Wybiera taÅ„sze modele
- âœ… Cache'uje odpowiedzi
- âœ… Limituje tokeny
- âœ… Loguje koszty

## ğŸ“ˆ Monitoring KosztÃ³w

### API Endpoint

```bash
curl http://localhost:8000/api/v1/stats/costs
```

**OdpowiedÅº:**
```json
{
  "status": "success",
  "data": {
    "total_cost": 0.1234,
    "total_requests": 250,
    "average_cost_per_request": 0.0005,
    "total_tokens": 125000,
    "estimated_monthly_cost": 15.00,
    "model_breakdown": {
      "gpt-4o-mini": {
        "requests": 200,
        "cost": 0.08,
        "input_tokens": 50000,
        "output_tokens": 50000
      },
      "gpt-4o": {
        "requests": 50,
        "cost": 0.0434,
        "input_tokens": 12500,
        "output_tokens": 12500
      }
    }
  },
  "message": "ğŸ’° Total cost: $0.1234"
}
```

### Logi Real-time

W logach backendu:
```
ğŸ’° LLM Cost: $0.000375 | Model: gpt-4o-mini | Tokens: 500â†’500 | Total: $0.1234
```

## ğŸ›ï¸ Konfiguracja

### Per-Agent Models

MoÅ¼esz wymusiÄ‡ konkretny model dla agenta:

```python
# backend/core/model_router.py

agent_models = {
    "planner": "default",       # gpt-4o-mini - wystarczy
    "mood": "default",          # gpt-4o-mini - jakoÅ›Ä‡ OK
    "decision": "smart",        # gpt-4o - potrzebuje reasoning
    "vision": "smart",          # gpt-4o - analiza obrazÃ³w
}
```

### Manual Override

WymuÅ› konkretny model w kodzie:

```python
response = await llm_engine.generate(
    prompt="...",
    force_model="gpt-4o"  # WymuÅ› droÅ¼szy model
)
```

### Disable Smart Routing

```bash
# .env
ENABLE_SMART_ROUTING=false
DEFAULT_MODEL=gpt-4o-mini  # Zawsze ten sam
```

## ğŸ’¾ Response Caching

Cache dziaÅ‚a automatycznie dla identycznych zapytaÅ„:

```python
# Pierwsze zapytanie - API call ($0.0004)
"Jaka jest stolica Polski?"

# Drugie zapytanie (w ciÄ…gu 1h) - z cache ($0.00!)
"Jaka jest stolica Polski?"
```

**Ustawienia:**
```bash
ENABLE_RESPONSE_CACHE=true
CACHE_TTL_SECONDS=3600  # 1 godzina
```

## ğŸ“‰ OszczÄ™dnoÅ›ci w Liczbach

### PrzykÅ‚ad: 100 zapytaÅ„/dzieÅ„

**PRZED optymalizacjÄ…** (gpt-4-turbo):
- Dziennie: $2-3
- MiesiÄ™cznie: **$60-90**
- Rocznie: **$720-1080**

**PO optymalizacji** (smart routing):
- Dziennie: $0.04-0.06 (95% gpt-4o-mini, 5% gpt-4o)
- MiesiÄ™cznie: **$1.20-1.80**
- Rocznie: **$14.40-21.60**

**OSZCZÄ˜DNOÅšCI: ~97% ($840/rok!)** ğŸ‰

### Z Cachingiem (+50% hit rate):

- MiesiÄ™cznie: **$0.60-0.90**
- Rocznie: **$7.20-10.80**

**OSZCZÄ˜DNOÅšCI: ~99% ($990/rok!)** ğŸš€

## ğŸ”§ Zaawansowane

### Custom Cost Limits

Dodaj daily cost limit:

```python
# backend/core/cost_tracker.py

DAILY_COST_LIMIT = 1.00  # $1/day

if cost_tracker.total_cost > DAILY_COST_LIMIT:
    raise Exception("Daily cost limit exceeded!")
```

### Cost Alerts

Email alert gdy koszt przekroczy threshold:

```python
from backend.core.cost_tracker import cost_tracker

if cost_tracker.total_cost > 10.00:
    send_email_alert(f"Cost exceeded $10: ${cost_tracker.total_cost}")
```

### Per-User Tracking

```python
# Rozszerz cost_tracker.py
cost_tracker.track_request(
    model="gpt-4o-mini",
    input_tokens=500,
    output_tokens=500,
    user_id="user123"  # Track per user
)
```

## ğŸ“ Best Practices

1. **UÅ¼ywaj gpt-4o-mini jako default** - wystarczy dla 95% zadaÅ„
2. **WÅ‚Ä…cz Smart Routing** - oszczÄ™dza bez straty jakoÅ›ci
3. **WÅ‚Ä…cz Caching** - zero-cost dla powtÃ³rzeÅ„
4. **Monitoruj koszty** - sprawdzaj /api/v1/stats/costs
5. **Limituj tokeny** - MAX_TOKENS_DEFAULT=1000
6. **Testuj z mockami** - development bez API
7. **Review model selection** - czy decision agent naprawdÄ™ potrzebuje gpt-4o?

## ğŸ› Troubleshooting

### Koszty nadal wysokie?

```bash
# SprawdÅº statystyki
curl http://localhost:8000/api/v1/stats/costs

# Zobacz ktÃ³ry model jest uÅ¼ywany
# Logi: "Model: gpt-4o-mini" = GOOD âœ…
# Logi: "Model: gpt-4-turbo" = BAD âŒ
```

### Smart routing nie dziaÅ‚a?

```bash
# SprawdÅº .env
grep ENABLE_SMART_ROUTING .env

# Powinno byÄ‡:
ENABLE_SMART_ROUTING=true
```

### Cache nie dziaÅ‚a?

```bash
# SprawdÅº logi - powinny byÄ‡ "Cache HIT!"
# JeÅ›li nie ma:

grep ENABLE_RESPONSE_CACHE .env
# Powinno byÄ‡:
ENABLE_RESPONSE_CACHE=true
```

## ğŸ“Š ROI Calculator

Oszacuj swoje oszczÄ™dnoÅ›ci:

```
Aktualne zapytania/dzieÅ„: _______
Åšredni koszt przed: $0.025
Åšredni koszt po: $0.0005

MiesiÄ™czne oszczÄ™dnoÅ›ci:
  (zapytania Ã— 30 Ã— $0.0245) = $_______

Roczne oszczÄ™dnoÅ›ci:
  (miesiÄ™czne Ã— 12) = $_______
```

## ğŸ¯ Roadmap

- [ ] Redis-based distributed caching
- [ ] Per-user cost limits
- [ ] Cost prediction model
- [ ] Auto-scaling based on budget
- [ ] Model performance tracking
- [ ] A/B testing model selection
- [ ] Cost anomaly detection

---

**ZredukowaliÅ›my koszty o 97%!** ğŸ’°âœ¨

Pytania? Zobacz [GitHub Issues](https://github.com/Marksio90/LumenAI-Life-Intelligence-System/issues)
