"""
Vision Agent - Image analysis, OCR, object detection
"""

from typing import Dict, Any, Optional, List
from loguru import logger
import base64
import io
from PIL import Image
import pytesseract
from pathlib import Path

from backend.agents.base import BaseAgent


class VisionAgent(BaseAgent):
    """
    Specialized agent for vision and image analysis
    - OCR (text extraction from images)
    - Object detection
    - Scene description
    - Image understanding with GPT-4V
    """

    def __init__(self, memory_manager=None):
        super().__init__(
            name="Vision",
            description="Analiza obrazÃ³w, OCR, rozpoznawanie obiektÃ³w i scen",
            memory_manager=memory_manager
        )

        # Try to set tesseract path for different environments
        try:
            # Try common paths
            for path in ['/usr/bin/tesseract', '/usr/local/bin/tesseract']:
                if Path(path).exists():
                    pytesseract.pytesseract.tesseract_cmd = path
                    break
        except Exception as e:
            logger.warning(f"Could not set tesseract path: {e}")

    async def process(
        self,
        user_id: str,
        message: str,
        context: Dict[str, Any],
        metadata: Optional[Dict] = None
    ) -> str:
        """Process vision-related requests"""

        logger.info(f"Vision Agent processing for {user_id}")

        # Check if image data is provided in metadata
        if not metadata or "image" not in metadata:
            return "ðŸ“¸ Aby przeanalizowaÄ‡ obraz, przeÅ›lij zdjÄ™cie wraz z pytaniem!"

        image_data = metadata.get("image")
        analysis_type = await self._determine_analysis_type(message)

        try:
            # Load image
            image = await self._load_image(image_data)

            if analysis_type == "ocr":
                return await self._perform_ocr(image, message)
            elif analysis_type == "object_detection":
                return await self._detect_objects(image, message)
            elif analysis_type == "scene_description":
                return await self._describe_scene(image, message)
            else:
                # General AI-powered analysis with GPT-4V
                return await self._analyze_with_vision_model(image, message, image_data)

        except Exception as e:
            logger.error(f"Vision processing error: {e}")
            return f"âŒ WystÄ…piÅ‚ bÅ‚Ä…d podczas analizy obrazu: {str(e)}"

    async def _determine_analysis_type(self, message: str) -> str:
        """Determine what type of vision analysis is needed"""

        message_lower = message.lower()

        # OCR keywords
        if any(word in message_lower for word in [
            "tekst", "przeczytaj", "odczytaj", "co jest napisane",
            "text", "read", "ocr"
        ]):
            return "ocr"

        # Object detection
        if any(word in message_lower for word in [
            "co widzisz", "obiekty", "rzeczy", "przedmioty",
            "what do you see", "objects", "detect"
        ]):
            return "object_detection"

        # Scene description
        if any(word in message_lower for word in [
            "opisz", "scena", "co siÄ™ dzieje", "describe", "scene"
        ]):
            return "scene_description"

        return "general"

    async def _load_image(self, image_data: Any) -> Image.Image:
        """Load image from various formats"""

        if isinstance(image_data, str):
            # Base64 encoded image
            if image_data.startswith('data:image'):
                # Remove data URL prefix
                image_data = image_data.split(',')[1]

            image_bytes = base64.b64decode(image_data)
            return Image.open(io.BytesIO(image_bytes))

        elif isinstance(image_data, bytes):
            return Image.open(io.BytesIO(image_data))

        elif isinstance(image_data, Image.Image):
            return image_data

        else:
            raise ValueError(f"Unsupported image data type: {type(image_data)}")

    async def _perform_ocr(self, image: Image.Image, message: str) -> str:
        """Extract text from image using OCR"""

        try:
            # Convert to grayscale for better OCR
            image_gray = image.convert('L')

            # Perform OCR with Polish and English
            text = pytesseract.image_to_string(image_gray, lang='pol+eng')

            if not text.strip():
                return "ðŸ“„ Nie wykryto Å¼adnego tekstu na obrazie. Upewnij siÄ™, Å¼e obraz zawiera czytelny tekst."

            # Let LLM format the response
            system_prompt = """
UÅ¼ytkownik poprosiÅ‚ o odczytanie tekstu z obrazu.
OtrzymaÅ‚eÅ› surowy tekst z OCR. Twoim zadaniem jest:
1. OczyÅ›ciÄ‡ tekst z artefaktÃ³w OCR
2. PoprawiÄ‡ formatowanie
3. OdpowiedzieÄ‡ na pytanie uÅ¼ytkownika dotyczÄ…ce tekstu

BÄ…dÅº pomocny i zwiÄ™zÅ‚y.
"""

            llm_response = await self._call_llm(
                prompt=f"Pytanie uÅ¼ytkownika: {message}\n\nOdczytany tekst:\n{text}",
                system_prompt=system_prompt
            )

            return f"ðŸ“„ **Odczytany tekst:**\n\n{llm_response}"

        except Exception as e:
            logger.error(f"OCR error: {e}")
            return f"âŒ BÅ‚Ä…d OCR: {str(e)}. Upewnij siÄ™, Å¼e Tesseract jest zainstalowany."

    async def _detect_objects(self, image: Image.Image, message: str) -> str:
        """Detect objects in image using AI vision model"""

        # Convert image to base64 for API
        image_base64 = await self._image_to_base64(image)

        system_prompt = """
JesteÅ› ekspertem od analizy obrazÃ³w.
UÅ¼ytkownik pyta o obiekty na zdjÄ™ciu.

Opisz szczegÃ³Å‚owo:
- Co widzisz na obrazie
- Jakie obiekty sÄ… obecne
- Ich poÅ‚oÅ¼enie i relacje
- Kolory, rozmiary, waÅ¼ne detale

BÄ…dÅº dokÅ‚adny ale naturalny w opisie.
"""

        return await self._analyze_with_vision_model(
            image, message, image_base64, system_prompt
        )

    async def _describe_scene(self, image: Image.Image, message: str) -> str:
        """Describe the scene in the image"""

        image_base64 = await self._image_to_base64(image)

        system_prompt = """
Opisz scenÄ™ na obrazie szczegÃ³Å‚owo:
- Co siÄ™ dzieje
- Kontekst i atmosfera
- Ludzie i ich aktywnoÅ›ci (jeÅ›li sÄ…)
- Otoczenie i tÅ‚o
- Emocje i nastrÃ³j sceny

Opisuj w sposÃ³b naturalny i angaÅ¼ujÄ…cy.
"""

        return await self._analyze_with_vision_model(
            image, message, image_base64, system_prompt
        )

    async def _analyze_with_vision_model(
        self,
        image: Image.Image,
        message: str,
        image_base64: str,
        custom_system_prompt: Optional[str] = None
    ) -> str:
        """Analyze image using OpenAI Vision API (GPT-4V)"""

        try:
            from backend.core.llm_engine import LLMEngine
            from backend.shared.config.settings import settings
            import openai

            # Initialize OpenAI client
            client = openai.AsyncOpenAI(api_key=settings.OPENAI_API_KEY)

            system_prompt = custom_system_prompt or """
JesteÅ› inteligentnym asystentem z moÅ¼liwoÅ›ciÄ… analizy obrazÃ³w.
Odpowiadaj na pytania uÅ¼ytkownika dotyczÄ…ce obrazu w sposÃ³b:
- DokÅ‚adny i szczegÃ³Å‚owy
- Pomocny
- Naturalny w jÄ™zyku
            """

            # Prepare image for API
            if not image_base64.startswith('data:image'):
                # Get image format
                img_format = image.format or 'PNG'
                mime_type = f"image/{img_format.lower()}"
                image_base64 = f"data:{mime_type};base64,{image_base64}"

            # Call GPT-4V
            response = await client.chat.completions.create(
                model="gpt-4o-mini",  # Using gpt-4o-mini for vision
                messages=[
                    {
                        "role": "system",
                        "content": system_prompt
                    },
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": message
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": image_base64
                                }
                            }
                        ]
                    }
                ],
                max_tokens=1000
            )

            result = response.choices[0].message.content
            return f"ðŸ” **Analiza obrazu:**\n\n{result}"

        except Exception as e:
            logger.error(f"Vision model error: {e}")
            return f"âŒ BÅ‚Ä…d podczas analizy AI: {str(e)}"

    async def _image_to_base64(self, image: Image.Image) -> str:
        """Convert PIL Image to base64 string"""

        buffered = io.BytesIO()

        # Convert RGBA to RGB if necessary
        if image.mode == 'RGBA':
            image = image.convert('RGB')

        image.save(buffered, format="PNG")
        img_str = base64.b64encode(buffered.getvalue()).decode()

        return img_str

    async def can_handle(self, message: str, context: Dict) -> float:
        """Check if this agent should handle the message"""

        vision_keywords = [
            "obraz", "zdjÄ™cie", "foto", "screen", "screenshot",
            "co widzisz", "przeczytaj", "tekst na", "opisz zdjÄ™cie",
            "image", "photo", "picture", "ocr", "read"
        ]

        message_lower = message.lower()
        matches = sum(1 for keyword in vision_keywords if keyword in message_lower)

        # High confidence if metadata contains image
        if context.get("has_image"):
            return 0.9

        return min(matches * 0.3, 0.8)
