"""
ChromaDB Service - Semantic Memory & Vector Search dla LumenAI

ChromaDB przechowuje embeddings (wektory) wiadomoÅ›ci, co pozwala na:
- Semantyczne wyszukiwanie ("znajdÅº rozmowy o pracy")
- Rekomendacje podobnych rozmÃ³w
- Grupowanie tematyczne
- Kontekst z przeszÅ‚ych rozmÃ³w

Architektura:
1. KaÅ¼da wiadomoÅ›Ä‡ â†’ embedding (1536-wymiarowy wektor)
2. ChromaDB przechowuje embedding + metadata
3. Wyszukiwanie przez similarity search
"""

import chromadb
try:
    from chromadb.config import Settings
except ImportError:
    # ChromaDB 0.5+ uses Settings from main module
    from chromadb import Settings
from typing import List, Dict, Optional, Any
from datetime import datetime
from loguru import logger
import hashlib


class ChromaDBService:
    """
    Service do zarzÄ…dzania embeddings i semantic search.

    Collections:
    - lumenai_messages: Wszystkie wiadomoÅ›ci uÅ¼ytkownikÃ³w
    - lumenai_conversations: Podsumowania rozmÃ³w
    """

    def __init__(
        self,
        host: str = "localhost",
        port: int = 8001,
        collection_name: str = "lumenai_messages"
    ):
        """
        Inicjalizacja ChromaDB.

        Args:
            host: Host ChromaDB (default: localhost)
            port: Port ChromaDB (default: 8001)
            collection_name: Nazwa kolekcji (default: lumenai_messages)
        """
        self.host = host
        self.port = port
        self.collection_name = collection_name
        self.client: Optional[chromadb.HttpClient] = None
        self.collection = None

    async def connect(self):
        """PoÅ‚Ä…cz z ChromaDB"""
        try:
            # ChromaDB HTTP Client
            self.client = chromadb.HttpClient(
                host=self.host,
                port=self.port,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )

            # Testuj poÅ‚Ä…czenie
            self.client.heartbeat()

            # Pobierz lub utwÃ³rz kolekcjÄ™
            self.collection = self.client.get_or_create_collection(
                name=self.collection_name,
                metadata={"description": "LumenAI message embeddings for semantic search"}
            )

            logger.info(f"âœ… ChromaDB connected: {self.host}:{self.port}")
            logger.info(f"ðŸ“š Collection '{self.collection_name}' ready ({self.collection.count()} documents)")
            return True

        except Exception as e:
            logger.warning(f"âš ï¸  ChromaDB connection failed: {e}")
            logger.warning("ðŸ”„ Running without vector search capabilities")
            return False

    async def disconnect(self):
        """RozÅ‚Ä…cz z ChromaDB"""
        if self.client:
            self.client = None
            logger.info("ChromaDB disconnected")

    async def health_check(self) -> bool:
        """SprawdÅº czy ChromaDB dziaÅ‚a"""
        try:
            if self.client:
                self.client.heartbeat()
                return True
        except:
            pass
        return False

    async def add_message(
        self,
        message_id: str,
        user_id: str,
        content: str,
        embedding: List[float],
        metadata: Optional[Dict[str, Any]] = None
    ) -> bool:
        """
        Dodaj wiadomoÅ›Ä‡ z embeddingiem do ChromaDB.

        Args:
            message_id: Unikalny ID wiadomoÅ›ci
            user_id: ID uÅ¼ytkownika
            content: TreÅ›Ä‡ wiadomoÅ›ci
            embedding: Wektor embedding (1536-wymiarowy dla OpenAI)
            metadata: Dodatkowe metadane (timestamp, agent, conversation_id, etc.)

        Returns:
            True jeÅ›li sukces, False jeÅ›li bÅ‚Ä…d
        """
        if not self.collection:
            logger.warning("ChromaDB not connected, skipping embedding storage")
            return False

        try:
            # Przygotuj metadane
            meta = metadata or {}
            meta.update({
                "user_id": user_id,
                "timestamp": datetime.utcnow().isoformat(),
                "content_length": len(content)
            })

            # Dodaj do ChromaDB
            self.collection.add(
                ids=[message_id],
                embeddings=[embedding],
                documents=[content],
                metadatas=[meta]
            )

            logger.debug(f"ðŸ“ Added message {message_id} to ChromaDB")
            return True

        except Exception as e:
            logger.error(f"Failed to add message to ChromaDB: {e}")
            return False

    async def search_similar(
        self,
        query_embedding: List[float],
        user_id: Optional[str] = None,
        n_results: int = 10,
        where: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Wyszukaj podobne wiadomoÅ›ci uÅ¼ywajÄ…c similarity search.

        Args:
            query_embedding: Embedding zapytania
            user_id: Opcjonalnie filtruj po user_id
            n_results: Ile wynikÃ³w zwrÃ³ciÄ‡
            where: Dodatkowe filtry (np. {"agent": "mood"})

        Returns:
            Lista podobnych wiadomoÅ›ci z metadanymi i dystansem
        """
        if not self.collection:
            logger.warning("ChromaDB not connected")
            return []

        try:
            # Przygotuj filtry
            where_filter = where or {}
            if user_id:
                where_filter["user_id"] = user_id

            # Wyszukaj
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                where=where_filter if where_filter else None,
                include=["documents", "metadatas", "distances"]
            )

            # Formatuj wyniki
            formatted = []
            if results and results["ids"]:
                for i, doc_id in enumerate(results["ids"][0]):
                    formatted.append({
                        "id": doc_id,
                        "content": results["documents"][0][i],
                        "metadata": results["metadatas"][0][i],
                        "distance": results["distances"][0][i],
                        "similarity": 1 - results["distances"][0][i]  # Convert distance to similarity
                    })

            logger.debug(f"ðŸ” Found {len(formatted)} similar messages")
            return formatted

        except Exception as e:
            logger.error(f"Search failed: {e}")
            return []

    async def search_by_text(
        self,
        query_text: str,
        user_id: Optional[str] = None,
        n_results: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Wyszukaj podobne wiadomoÅ›ci uÅ¼ywajÄ…c tekstu (bez rÄ™cznego embeddingu).

        UWAGA: Ta metoda wymaga wygenerowania embeddingu dla query_text
        najpierw przez OpenAI API. UÅ¼yj search_similar() zamiast tego.

        Args:
            query_text: Tekst zapytania
            user_id: Opcjonalnie filtruj po user_id
            n_results: Ile wynikÃ³w zwrÃ³ciÄ‡

        Returns:
            Lista podobnych wiadomoÅ›ci
        """
        # To bÄ™dzie zaimplementowane po dodaniu embedding service
        logger.warning("search_by_text requires embedding service - use search_similar instead")
        return []

    async def get_conversation_context(
        self,
        user_id: str,
        query_embedding: List[float],
        n_results: int = 5
    ) -> str:
        """
        Pobierz kontekst z podobnych przeszÅ‚ych rozmÃ³w.

        UÅ¼yj tego aby daÄ‡ LLM kontekst z przeszÅ‚oÅ›ci:
        "UÅ¼ytkownik wczeÅ›niej rozmawiaÅ‚ o podobnych tematach..."

        Args:
            user_id: ID uÅ¼ytkownika
            query_embedding: Embedding obecnej wiadomoÅ›ci
            n_results: Ile kontekstÃ³w zwrÃ³ciÄ‡

        Returns:
            Sformatowany string z kontekstem
        """
        similar = await self.search_similar(
            query_embedding=query_embedding,
            user_id=user_id,
            n_results=n_results
        )

        if not similar:
            return "No similar past conversations found."

        # Formatuj kontekst
        context_parts = []
        for item in similar:
            context_parts.append(
                f"[{item['metadata'].get('timestamp', 'unknown')}] "
                f"{item['content'][:100]}... (similarity: {item['similarity']:.2f})"
            )

        return "Similar past conversations:\n" + "\n".join(context_parts)

    async def delete_user_data(self, user_id: str) -> bool:
        """
        UsuÅ„ wszystkie embeddingi uÅ¼ytkownika (GDPR compliance).

        Args:
            user_id: ID uÅ¼ytkownika

        Returns:
            True jeÅ›li sukces
        """
        if not self.collection:
            return False

        try:
            # Pobierz wszystkie ID uÅ¼ytkownika
            results = self.collection.get(
                where={"user_id": user_id},
                include=[]
            )

            if results and results["ids"]:
                self.collection.delete(ids=results["ids"])
                logger.info(f"ðŸ—‘ï¸  Deleted {len(results['ids'])} embeddings for user {user_id}")
                return True

        except Exception as e:
            logger.error(f"Failed to delete user data: {e}")

        return False

    def get_stats(self) -> Dict[str, Any]:
        """Pobierz statystyki kolekcji"""
        if not self.collection:
            return {"status": "disconnected"}

        try:
            count = self.collection.count()
            return {
                "status": "connected",
                "collection": self.collection_name,
                "documents": count,
                "host": f"{self.host}:{self.port}"
            }
        except Exception as e:
            return {"status": "error", "error": str(e)}


# ============================================================================
# Singleton Pattern - Jedna instancja dla caÅ‚ej aplikacji
# ============================================================================

_chromadb_service: Optional[ChromaDBService] = None


def init_chromadb_service(
    host: str = "localhost",
    port: int = 8001,
    collection_name: str = "lumenai_messages"
) -> ChromaDBService:
    """Inicjalizuj globalny ChromaDB service"""
    global _chromadb_service

    _chromadb_service = ChromaDBService(
        host=host,
        port=port,
        collection_name=collection_name
    )

    return _chromadb_service


def get_chromadb_service() -> Optional[ChromaDBService]:
    """Pobierz globalny ChromaDB service"""
    return _chromadb_service
